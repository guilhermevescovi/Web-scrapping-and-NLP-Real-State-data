{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Does basic house information reflect house's description?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T17:51:25.137584Z",
     "start_time": "2018-11-28T17:51:25.091710Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\facch\\Desktop\\DS\\ADM\\Homework4\\data_downloader.py:14: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 14 of the file C:\\Users\\facch\\Desktop\\DS\\ADM\\Homework4\\data_downloader.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = bs4.BeautifulSoup(requests.get(url).text)\n",
      "C:\\Users\\facch\\Desktop\\DS\\ADM\\Homework4\\data_downloader.py:51: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 51 of the file C:\\Users\\facch\\Desktop\\DS\\ADM\\Homework4\\data_downloader.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  page_soup = bs4.BeautifulSoup(requests.get(link).text)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fc211e1a0e7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_downloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdescriptions_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlist_features_all_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_downloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\DS\\ADM\\Homework4\\data_downloader.py\u001b[0m in \u001b[0;36mdownloader\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0mlink\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"href\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[0mpage_soup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbs4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                 \u001b[0mdescription\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpage_soup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"class\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"col-xs-12 description-text text-compressed\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdescription\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                     \u001b[0mdescriptions_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import data_downloader\n",
    "descriptions_list,list_features_all_c = data_downloader.downloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('raw_data.pkl', 'wb') as f:\n",
    "    pickle.dump((descriptions_list,list_features_all_c), f, pickle.HIGHEST_PROTOCOL)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('raw_data.pkl', 'rb') as f:\n",
    "    loaded = pickle.load(f)\n",
    "    list_features = loaded[1]\n",
    "    list_description = loaded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tfIdf_functions\n",
    "df = tfIdf_functions.tfIdf_calculator(tfIdf_functions.cleaner(list_description))\n",
    "description_matrix = np.nan_to_num(df.values)\n",
    "features_matrix = np.array(list_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('processed_data.pkl', 'wb') as f:\n",
    "    pickle.dump((description_matrix,features_matrix), f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start to build the actual cluster we need to prepare the data. The most important thing is to check how many cluster we should use in our clustering, and to do so we wish to use the elbow method.\n",
    "\n",
    "First of all let us load our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('processed_data.pkl', 'rb') as f:\n",
    "    loaded = pickle.load(f)\n",
    "    features_matrix = loaded[1]\n",
    "    description_matrix = loaded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18468\n"
     ]
    }
   ],
   "source": [
    "print(len(description_matrix[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unluckly the description matrix has a very huge number of features, therefore the computation will be very slow, in the order of days of computation, to find an optimal cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For solve this problem we will apply a method of dimension reduction to our data to get a more usable amount of data. In particular, we will use the principal component reduction method. It is a widly used method in statistic to reduce the dimension of random variable without losing too much information. In particular we will set a treshold value of $0.95$ for the variance ratio of our data. That is, we will check how much is the variance rate given by our PCA reduction for different dimensions and choose the first we found having a variance ratio of $0.95$. That means that we are losing $5$% of our initial information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0.5800906035782916\n",
      "2000 0.7373358809031546\n",
      "3000 0.8284489289787839\n",
      "4000 0.888605939746952\n",
      "5000 0.9300534448214751\n",
      "6000 0.9592126712259268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "for n in range(1000,len(description_matrix[0]),1000):\n",
    "    pca = PCA(n_components=n).fit(description_matrix)\n",
    "    value = np.cumsum(pca.explained_variance_ratio_)\n",
    "    print(n, value[-1])\n",
    "    if value[-1] > 0.95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we reduced the dimension of our space from $18468$ to only $6000$ losing a $5$% of precision. We now overwrite our matrx with the reducted one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=6000)\n",
    "description_matrix = pca.fit_transform(description_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we will now save our new data for future uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('pca_data.pkl', 'wb') as f:\n",
    "    pickle.dump((description_matrix,features_matrix), f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('processed_data.pkl', 'rb') as f:\n",
    "    loaded = pickle.load(f)\n",
    "    features_matrix = loaded[1]\n",
    "    description_matrix = loaded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mms = StandardScaler()\n",
    "mms.fit(description_matrix)\n",
    "data_transformed = mms.transform(description_matrix)\n",
    "\n",
    "Sum_of_squared_distances = []\n",
    "K = range(100,1000,100)\n",
    "for k in K:\n",
    "    print(k)\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(data_transformed)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "    \n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mms.fit(features_matrix)\n",
    "data_transformed = mms.transform(features_matrix)\n",
    "\n",
    "Sum_of_squared_distances = []\n",
    "K = range(1,30)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(data_transformed)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "    \n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster\n",
    "clustered_features = sklearn.cluster.KMeans(n_clusters=7).fit(features_matrix)\n",
    "clustered_descriptions = sklearn.cluster.KMeans(n_clusters=40).fit(description_matrix)\n",
    "features_centers = clustered_features.cluster_centers_\n",
    "descriptions_centers = clustered_descriptions.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_indices(value, qlist):\n",
    "    indices = []\n",
    "    idx = -1\n",
    "    while True:\n",
    "        try:\n",
    "            idx = qlist.index(value, idx+1)\n",
    "            indices.append(idx)\n",
    "        except ValueError:\n",
    "            break\n",
    "    return set(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cluster_list = []\n",
    "for i in range(7):\n",
    "    features_cluster_list.append(all_indices(i,list(clustered_features.labels_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "descriptions_cluster_list = []\n",
    "for i in range(40):\n",
    "    descriptions_cluster_list.append(all_indices(i,list(clustered_descriptions.labels_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobi_sim(S1,S2):\n",
    "    intl = len(S1.intersection(S2))\n",
    "    unil = len(S1.union(S2))\n",
    "    return intl/unil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobi_sim_matrix(setlist1, setlist2):\n",
    "    results_list = []\n",
    "    for S1 in setlist1:\n",
    "        l = []\n",
    "        for S2 in setlist2:\n",
    "            l.append(jacobi_sim(S1,S2))\n",
    "        results_list.append(l)\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = jacobi_sim_matrix(features_cluster_list,descriptions_cluster_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
